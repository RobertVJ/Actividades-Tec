# -*- coding: utf-8 -*-
"""Implementacion_de_tecnica_de_IA_sin_Framework.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zcFlzIOZtlUbbZvZaHWSmyoHyXKZogvT

#  Implementación de una técnica de aprendizaje máquina sin el uso de un framework

Autor: Roberto Valdez Jasso

Matricula: A01746863

Fecha de Inicio 30/08/2022

Fecha de Finalizacion: 2/09/2022

Entregable: Implementación de una técnica de aprendizaje máquina sin el uso de un framework.

1. Crea un repositorio de GitHub para este proyecto.


2. Programa uno de los algoritmos vistos en el módulo (o que tu profesor de módulo autorice) sin usar ninguna biblioteca o framework de aprendizaje máquina, ni de estadística avanzada. Lo que se busca es que implementes 
manualmente el algoritmo, no que importes un algoritmo ya implementado. 

3. Prueba tu implementación con un set de datos y realiza algunas predicciones. Las predicciones las puedes correr en consola o las puedes implementar con una interfaz gráfica apoyándote en los visto en otros módulos.

4. Tu implementación debe de poder correr por separado solamente con un compilador, no debe de depender de un IDE o de un “notebook”. Por ejemplo, si programas en Python, tu implementación final se espera que esté en un archivo .py no en un Jupyter Notebook.

5. Después de la entrega intermedia se te darán correcciones que puedes incluir en tu entrega final.

# Modelo  de Inteligencia Artificial Elegido

El Modelo Elegido para esta actividad es: Regresion Logistica.



Nota:
En esta actividad usare  un codigo que ya realice en cursos anteriores realizados en Linkedin con la finalidad de ver y comparar el porcentaje de acercamiento del realizado con librerias como tambien los valores pruebas regresado con una semilla random diferente al anterior.
"""

# Librerias base para generar la la regresion Logistica 
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import random
import math
import  seaborn as sb
from google.colab import drive
from  sklearn.preprocessing import  LabelEncoder
from  sklearn.preprocessing import  OneHotEncoder
from  sklearn.model_selection import train_test_split

# Commented out IPython magic to ensure Python compatibility.
# Conexion con Drive para recibir los dataset del
drive.mount("/content/gdrive")  
!pwd  # show current path 

# Revision del los datos disponibles (csv) en la carpeta
# %cd "/content/gdrive/MyDrive/Data/"
!ls  # show current directory

# Dataframe titanic

# Generamos la el Dataframe/Dataset 
# Con el csv de automobile csv

titanic_training = pd.read_csv('titanic.csv')

#Una vez ya creado, vemos los primeros 10  datos de la tabla
titanic_training.head(10)

"""# Procesado de los datos disponibles"""

#Revisamos la informacion del dataset titanico
titanic_training.info()

"""
Descripcion de las variables

Survived: Survival (0 = NO, 1= YES) (variable binaria)
pClass = Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)
Name = Name
Sex = Sex
Age = Age
SibSp = Number of Siblings/Spouses Aboard
Parch = Number of Parents/Chindren Aboard
Ticket = Number of Tickets
Fare = Passanger Fare (British Pound)
Cabin = Cabin
Embarked = Port of Embarkation ( C = Cherbour ,France;
                                 Q = Queenstown, UK;
                                 S = Southampton, Cobh- Ireland;                                            
                                ) variable categorica
"""

# DATA PREPARATON

# para preparar los datos primero necetamos checar si la variable tarjet es binaria
# Queremos checar, probar y predecir la sobrevencia , pero primero hay que checar si es binario
#sb.countplot(x = "Survived", data = titanic_training, palette = 'hls')
#plt.show() # vemos la grafica de sobreviencia denotando que si es binaria la variable

# Checamos por valores perdidos
# Siempre hay que hacerlo (en este caso si hay valores perdidos , lo podemos checar desde la info del dataset)
titanic_training.isnull()
sum = titanic_training.isnull().sum()
sum

# Descricion de los datos
titanic_training.describe()
"""
Taking care of missing values
    Dropping missing values
    So let's just go ahead and drop all the variables that aren't relevant for predicting survival. We should at least keep the following:
    Survived - This variable is obviously relevant.
    Pclass - Does a passenger's class on the boat affect their survivability?
    Sex - Could a passenger's gender impact their survival rate?,
    Age - Does a person's age impact their survival rate?
    SibSp - Does the number of relatives on the boat (that are siblings or a spouse) affect a person survivability? Probability
    Parch - Does the number of relatives on the boat (that are children or parents) affect a person survivability? Probability
    Fare - Does the fare a person paid effect his survivability? Maybe - let's keep it.
    Embarked - Does a person's point of embarkation matter? It depends on how the boat was filled... Let's keep it

    What about a person's name, ticket number, and passenger ID number? They're irrelavant for predicting survivability.
    And as you recall, the cabin variable is almost all missing values, so we can just drop all of these.
"""

# Generamos un nuevo dataframe con la elimanacion de los campos anteriores
titanic_data = titanic_training.drop(['Name', 'Ticket', 'Cabin'], axis= 1) # tumbamos las columnas
titanic_data.head() # dataset limpio con datos relevantes

# tratando con valores desaparecidos
# Agregamos los valores desaparecidos con aproximacion

# Primero checamos la distribuccion de los datos entre Parch y Age
sb.boxplot(x = "Parch",y= "Age", data = titanic_data, palette = 'hls')
plt.show() # podemos denotar que hay relaciones entre ambas variables, es decir, podemos ver  que puede haber dos relativos por bote
           # (mientras mas edad mauor cantidad hasta llegar poren encima de 40 años)

# Aproximacion de las edades de los pasajeros en base al numero de padres o hijos que hay en el bote
# generamos un dataframe para la aproximacion

Parch_groups = titanic_data.groupby(titanic_data['Parch'])
Parch_groups.mean() # Sacamos la media de checar las edades
                            # por categoria tenemos la mediade las edades
                            # para las personas que tenga cero hijos o padres en el barco la edad promedio es de 32
                            # para las personas que tenga un hijo o padre en el barco la edad promedio es de 24
                            # para las personas que tenga dos hijos o padres en el barco la edad promedio es de 17
                            # para las personas que tenga tres hijos o padres en el barco la edad promedio es de 33
                            # para las personas que tenga cuatro hijos o padres en el barco la edad promedio es de 44
                            # para las personas que tenga cinco hijos o padres en el barco la edad promedio es de 39
                            # para las personas que tenga seis hijos o padres en el barco la edad promedio es de 43

def age_aprox(cols):
    Age = cols[0]
    Parch = cols[1]
    # Checamos si  los valores esta vacios o no existen
    if pd.isnull(Age):
        # Si es asi usamos la aproximacion de grupos de hijos o padre por
        # media de edades de los pasajeros en el barco como esta arriba
        if Parch == 0:
            return 32
        elif Parch == 1:
            return  24
        elif Parch == 2:
            return  17
        elif Parch == 3:
            return  33
        elif Parch == 4:
            return  44
        else:
            return  39 # 5 hijos o padres
    else:
        return Age

# Agregamos los datos por aproximacion
titanic_data['Age'] = titanic_data[['Age', 'Parch']].apply(age_aprox, axis = 1)

# Checamos que no haya vacios (cosa ya no habra)
titanic_data.isnull().sum()

# tumbamos las filas pertenecientes a eso dos valores perdidos
titanic_data.dropna(inplace = True)
titanic_data.reset_index(inplace = True, drop= True)
#Revisamos la informacion del dataset
titanic_data.info() # imprimos la informacion

# RE-ENCODE VARIABLES

# Covertiendo variables categoricas a indicadores dummy

# vamos a reformatear la variable sex y embarked a variables numericas

# Hacemos esto para que funcionen dentro del modelo

label_encoder = LabelEncoder() # llamamos al encoder
# convertimos el genero
gender_Cat = titanic_data['Sex']
gender_Encoder = label_encoder.fit_transform(gender_Cat) # encoder la seccion sex
gender_Encoder[0:5] # no sabemos que signifca 1 o 0
# creando un gender Dataframe
gender_DF = pd.DataFrame(gender_Encoder, columns= ['male_gender'])
gender_DF.head()

# Ahora trabajaremos embarked
embarked_Cat = titanic_data['Embarked']
embarked_encoder=  label_encoder.fit_transform(embarked_Cat) # indicadores nuevos dummy

# hacemos lo siguiente para conseguir una variable binaria de embarked
# necesitamo one hot encoder
binary_encoder = OneHotEncoder( categories= 'auto')
embarked_1hot = binary_encoder.fit_transform(embarked_encoder.reshape(-1,1)) # genera un array de una sola columna pero la queremos en matrix
embarked_1hot_mat = embarked_1hot.toarray() # para matrix
embarked_DF  = pd.DataFrame(embarked_1hot_mat, columns= ['C', 'Q', 'S']) #de la matrix a dataframe, las columnas representas en donde embarcaron
embarked_DF.head()

# tiramos para abajo  las columnas ya no necesarias de Titanic data

titanic_data.drop(['Sex', 'Embarked'], axis = 1, inplace = True)
titanic_data.head()

# ahora concatenamos los nuevos indicadores a la tabla original
titanic_data_dmy = pd.concat([titanic_data,gender_DF,embarked_DF], axis= 1, verify_integrity= True).astype(float)
titanic_data_dmy.head()

# VALIDATING DATASET

# Checando por independecias entre atributos

plt.figure()
sb.heatmap(titanic_data_dmy.corr()) # corelacionamos los datos
plt.show() # Lo que nos dice la grafica que esu si tenemos corelacion cerca a uno o uno negtaivos, significa que
           # se obtuvo una fuerte relacion lineal entre el par de varaibles
           # regresion logistica asume que los atributos debe ser independiente del uno con el otro
           # lo cual no podemos tener esto

# Fare y Plass no son independiente de uno y del otro vamos tumbarlas
titanic_data_dmy.drop(['Fare', 'Pclass'], axis= 1, inplace= True)
titanic_data_dmy.head() # datafame mas limpio

# MODEL DEPLOYMENT
# Rompemos el dataframe para  el set entrenamiento (4/5 de datos del dataframe) y  set pruebas (pass set) (1/5 de datos del dataframe)
# y quitamos la variable Survived qque es la que queremos checar
X_train, X_test, Y_train, Y_test = train_test_split(titanic_data_dmy.drop(['Survived'], axis= 1), # Valores en X
                                                    titanic_data_dmy['Survived'], test_size = 0.2, # Valores en Y
                                                    random_state= 0) # seed de random para tener los mismos resultados

#checando los resultados
print(f'Valores de entrenamiento en X :\n{X_train.shape} \nvalores en Y:\n {Y_train}') # En y tenemos una ifla con 711 datos y en
print("/------------------------------------/")

#checando los resultados
print(f'Valores de entrenamiento predictores en X :\n{X_train[0:5]}')  #Checamos por los primeros 5 valores que usaramos como predictores
print("/------------------------------------/")

"""# Generacion del Modelo de aprendizaje

Debido a la actividad, en este codigo se encontraran el  codigo necesario para realizar la regresion Logistica sin librerias de IA y estadisticas avanzadas
"""

# Regresion Logistica a "mano"

#Clase Regresion Logistica

class LogitRegression() :
    def __init__( self, learning_rate, epochs ) :        
        self.learning_rate = learning_rate # ratio de aprendizaje        
        self.epochs = epochs # interaciones del modelo 
            
    # Funcion fit 
    # Nos apoya generando el modelo para entrenar la regresion logistica
    # como tambien realizarla.
    def fit( self, X, Y ) :        
        # Variables iniciales       
        self.m, self.n = X.shape  # toman en tamaño (shape) de los datos de entrenamiento en  
                                  # Estos deben ser de manera obligatoria datos de entrenamiento
                                  # y no de los de preuba, en caso contrario no funcionara el
                                  # modelo.    
        # Inicializacion de los pesos       
        self.W = np.zeros( self.n )  # peso inicial      
        self.b = 0  #  Bias inicial    
        self.X = X  # Datos de entrenamiento en X  (vector)     
        self.Y = Y  # Datos de entrenamiento en Y
          
        # Ciclo for para generar el aprendizaje
        # del gradiente descendiente.
        # se genera por las epocas decidas por el usuario          
        for i in range( self.epochs ) :            
            self.calculate_weights()  # Caluclo constante de los pesos          
        return self
      
    # Funcion calculate weights
    # Funcion que nos apoya en generar los pesos
    # al momento de calcular el gradiente descendiente 
    def calculate_weights( self ) :          
        A = 1 / ( 1 + np.exp( - ( self.X.dot( self.W ) + self.b ) ) ) # Alfa (aprendizaje)
        # Se calculan el gradiente desencidiente       
        tmp = ( A - self.Y.T )        
        tmp = np.reshape( tmp, self.m )        
        dW = np.dot( self.X.T, tmp ) / self.m          
        db = np.sum( tmp ) / self.m 
        # se actualizan los nuevos pesos con el ratio de aprendizaje   
        self.W = self.W - self.learning_rate * dW    
        self.b = self.b - self.learning_rate * db
          
        return self # regresamos la carga nueva de pesos 
      
    # Funcion Predict
    # Genera la prediccion del modelo, en base a la formula
    # de funcion hipotetica.  
    def predict( self, X ) :    
        Z = 1 / ( 1 + np.exp( - ( X.dot( self.W ) + self.b ) ) )  # sigmoide       
        Y = np.where( Z > 0.5, 1, 0 )        
        return Y # regresamos la prediccion

# Llamando al modelo   
# Model training    
model = LogitRegression( learning_rate = 0.01, epochs = 100000 ) # Si Funciona pero necesita muchas MUCHAS Interaciones como tambien el      
model.fit( X_train, Y_train )  # tarda su rato dependiendo las epocas y el ratio de apredizaje que se le ponga   
    
# Prediction on test set
Y_pred = model.predict( X_test )    
      
print(f'Prediccion Modelo realizado:\n {Y_pred}')  #Checamos por los primeros 5 valores que usaramos como predictores
print("/------------------------------------/")

# Librerias para ver el modelo con libreria
from  sklearn.linear_model import LogisticRegression
import warnings
warnings.filterwarnings( "ignore" )

#modelo con libreria 
# Realizamos este modelo para ver la  realizacion del modelo generado
# para la actividad como tambien la precision de ambos modelos
model1 = LogisticRegression()    
model1.fit( X_train, Y_train)
Y_pred1 = model1.predict( X_test )
print(f'Prediccion con Libreria :\n {Y_pred1}')  # con facilidad podemos decir que esta es mas rapida
print("/------------------------------------/")

# Librerias que nos apoya para generar reportes de clasificacion
# generar
from  sklearn.metrics import  classification_report

# Reporte de Clasificacion  sin cross validation del modelo generado
print(f'Reporte de clasificacion sin crossvalidation :\n {classification_report(Y_test, Y_pred)}') # vemos 43 de presicion de predicion que nada bueno pero puede mejorar
                                                                                                   # modificando el learning ratio y las epocas realizadas
print("/------------------------------------/")

# Reporte de Clasificacion  sin cross validation del modelo con libreria
print(f'Reporte de clasificacion  :\n {classification_report(Y_test, Y_pred1)}') # vemos 72 de presicion de predicion que si decente pero puede mejorar
print("/------------------------------------/")